{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: é™³æ¥“ç¿”\n",
    "\n",
    "Student ID: 109062561\n",
    "\n",
    "GitHub ID: eric40424 (34576440)\n",
    "\n",
    "Kaggle name: Feng Hsiang Chen\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2020-Lab2-Master Repo](https://github.com/fhcalderon87/DM2020-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2020-hw2-nthu/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 5th 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM2020-Lab2-Homework](https://github.com/fhcalderon87/DM2020-Lab2-Homework) repository this time! Also please __DONÂ´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹å°‡Reportåˆ†æˆå››å€‹éƒ¨åˆ†:\n",
    "1. é¦–å…ˆæ˜¯æ•´é«”çš„æƒ³æ³•:\n",
    "   æˆ‘å…ˆå°‡è³‡æ–™åšè™•ç†ï¼Œè®Šæˆåƒæ˜¯Lab2ä¸€æ¨£å¯ä»¥ç”¨çš„DataFrameï¼Œç„¶å¾Œä¾åºä½¿ç”¨Dicision Treeã€Naive Bayesã€Deep Learningç­‰æ–¹æ³•å…ˆåšå‡ºåˆæ­¥çš„æˆæœï¼Œé€™    æ™‚å€™å¯ä»¥çœ‹å‡ºNaive Bayesçš„æ–¹æ³•åœ¨é€™å¹¾ç¨®æ–¹æ³•ä¸­æˆæœæ˜¯ä¸éŒ¯çš„ï¼Œæ–¼æ˜¯å°‡å®ƒçš„çµæœä¸Šå‚³åˆ°Kaggleä¸Šï¼Œå¾—åˆ°35%çš„æ­£ç¢ºç‡ã€‚æ¥è‘—æ˜¯æƒ³è¾¦æ³•æ‰¾å…¶ä»–æ›´é©åˆçš„        modleï¼Œæ–¼æ˜¯æˆ‘æ¥è‘—å˜—è©¦äº†LSVCå’ŒLSTM+Word2Vecé€™2ç¨®æ–¹æ³•ï¼Œå…¶ä¸­ä»¥LSVCçš„æ–¹æ³•æœ€å¥½ï¼Œæ–¼æ˜¯æˆ‘å°±ä»¥é€™å€‹modelä½œç‚ºåŸºç¤ï¼Œæƒ³è¾¦æ³•åšä¸€äº›preprocessingã€ \n",
    "   feautuer engineeringï¼Œè‡³æ–¼LSTMé‡åˆ°çš„å•é¡Œç­‰ç­‰æœƒèªªæ˜ã€‚\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "2. æ¥è‘—æ˜¯LSVCæ–¹æ³•çš„è©³ç´°éç¨‹:\n",
    "   (1) Preprocessing:æˆ‘å°‡åŸæœ¬çš„è³‡æ–™ç”¢ç”Ÿå‡ºraw textå¾Œç™¼ç¾æœ‰å¾ˆå¤š#è·Ÿ@é€™äº›ä¸å¿…è¦çš„å­—å…ƒï¼Œä¹Ÿæœ‰å¾ˆå¤šæ¨™é»ç¬¦è™Ÿï¼Œæ–¼æ˜¯æˆ‘æœƒå°‡é€™äº›ç¬¦è™Ÿå¾raw textä¸­ç§»      é™¤ï¼Œä»¥å…é€ æˆä¸å¥½çš„åˆ¤æ–·ï¼Œæ¥è‘—å› ç‚ºæœ‰å¾ˆå¤šwordå¯èƒ½æœ‰ä¸€æ¨£çš„æ„æ€åƒæ˜¯caringå’Œcareï¼Œæ‰€ä»¥æˆ‘æœƒç”¨nltkçš„WordNetLemmatizerå°å­—æ¯åšè™•ç†ï¼Œæ›æˆä¸€æ¨£çš„    å­—æ¯ï¼Œæ›´åˆ©æ–¼åˆ¤æ–·ã€‚é™¤æ­¤ä¹‹å¤–æˆ‘ä¹Ÿæœ‰å»æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡æˆ–éºå¤±çš„è³‡æ–™ï¼Œä¹Ÿå»æŸ¥äº†åŸæœ¬training setçš„emotionåˆ†å¸ƒï¼Œè®“æˆ‘å¯ä»¥å°æ•´å€‹Datasetæœ‰æ¯”è¼ƒå®Œæ•´çš„    äº†è§£ï¼Œé€™å¹«æˆ‘æœ€å¾Œæƒ³åˆ°æŠ•ç¥¨çš„æ–¹æ³•ã€‚\n",
    "   (2) Feauture engineering:æˆ‘æœƒå°‡texté€éTF-IDFçš„æ–¹å¼è½‰æˆdocumentï¼Œæœƒæ¯”åŸæœ¬Lab2çš„æ–¹å¼å¥½å¾ˆå¤šï¼Œé™¤æ­¤ä¹‹å¤–ä¹ŸåŒ…åˆ®one hot encodingçš„éƒ¨åˆ†ã€‚\n",
    "   (3) è§£é‡‹LSVC Model(é€™æ¬¡æœ€å¥½çš„Model):ä¸»è¦å°±æ˜¯åˆ©ç”¨Support Vectorçš„æ–¹å¼ä¾†è¨ˆç®—Marginï¼Œä¸¦ä½¿ä»–æœ€å¤§åŒ–ã€‚ä»–æœ‰ä»¥ä¸‹å„ªé»:\n",
    "       <1> å¯ä»¥è§£æ±ºé«˜ç¶­å•é¡Œ(ç¬¦åˆé€™æ¬¡ä½œæ¥­æƒ…æ³ï¼Œå› ç‚ºæˆ‘ç”¨TF-IDFç”¢ç”Ÿäº†å¤šç¶­çš„è³‡æ–™)\n",
    "       <2> è§£æ±ºå°æ¨£æœ¬æ©Ÿå™¨å­¸ç¿’å•é¡Œ\n",
    "       <3> èƒ½è™•ç†éç·šæ€§ç‰¹å¾µçš„ç›¸äº’ä½œç”¨\n",
    "       <4> æ²’æœ‰å±€éƒ¨æœ€å°å€¼çš„å•é¡Œ\n",
    "       <5> ç„¡é ˆä¾è³´æ•´å€‹æ•¸æ“š\n",
    "       è€Œæˆ‘çš„é¸æ“‡æ˜¯LinearSVCå’Œä¸€èˆ¬SVC(with linear kernel)çš„å·®åˆ¥æ˜¯ä¸€å€‹æ˜¯Hingelosså¹³æ–¹æœ€å°åŒ–ã€ä¸€å€‹æ˜¯æœ€å°åŒ–ï¼Œä½†æ˜¯ä¸€èˆ¬ç‹€æ³ä¸‹Linear SVCçš„æ–¹å¼çš„çµæœæœƒæ¯”è¼ƒå¥½ã€‚\n",
    "   åšå®Œé€™äº›å¾Œå°±åœ¨Kaggleä¸Šç²å¾—45%çš„æ­£ç¢ºç‡ã€‚\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "3. é€™è£¡æ˜¯LSTMé‡åˆ°çš„å•é¡Œ:æˆ‘åœ¨å˜—è©¦éMachine Learing Basedçš„æ–¹æ³•å¾Œï¼Œæœ¬ä¾†æƒ³å˜—è©¦ä½¿ç”¨Deep Learing Basedçš„æ–¹æ³•ï¼Œä½†æ˜¯é‡åˆ°äº†è¨±å¤šå•é¡Œï¼Œæˆ‘ç”¨          embedding layer + biLSTM layer + Dense + softmaxåšå‡ºä¸€å€‹çš„æ¨¡å‹ï¼Œé æœŸæœƒæ¯”Lab2çš„neural networkå¥½å¾ˆå¤šï¼Œé›–ç„¶ä¹Ÿåœ¨training accuracyå’Œ\n",
    "   validation accuracyçš„çµæœä¸Šéƒ½é”åˆ°50å¹¾%çš„æ­£ç¢ºç‡ï¼Œä½†æ˜¯ä¸Šå‚³å»overfittingäº†ï¼Œåªæœ‰38%å·¦å³ï¼Œè€Œä¸”trainingæ™‚é–“åˆéå¸¸ä¹…ï¼Œåœ¨æœ¬èº«ä¸æ˜¯é€™æ–¹é¢å°ˆæ¥­    çš„æƒ…æ³ä¸‹ï¼Œæœ€å¾Œåªå¥½æ”¾æ£„é€™å€‹æ–¹æ³•ã€‚é‚„æœ‰ä¸€å€‹å•é¡Œæ˜¯æœ¬èº«training dataå¤ªå¤šè©²å¦‚ä½•å–æ¨åˆæ˜¯ä¸€å€‹å•é¡Œã€‚\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "4. Different things you tried and insights you gained:\n",
    "   (1) ç•¶training dataçµ¦å¾—è¶Šå¤šï¼Œæ•´é«”çš„æº–ç¢ºç‡æœƒè¶Šé«˜ï¼Œé›–ç„¶training accuracyå’Œtesting accuracyå·®è·æœƒè¶Šå¤§ã€‚\n",
    "   (2) å˜—è©¦ä½¿ç”¨wiki-news-300d-1M.vecé€™å€‹word2vecï¼Œçµæœç™¼ç¾å®ƒçš„å­—ä¸å¤ ï¼Œæœƒæœ‰å¾ˆå¤šæ²’çœ‹éçš„å­—ï¼Œå¯èƒ½æœƒä¸å‡†ï¼Œæ‰€ä»¥ä½¿ç”¨googleçš„vectoræœƒæ¯”è¼ƒå¥½ã€‚\n",
    "   (3) ä¸»è¦æ˜¯ç¬¬ä¸€æ¬¡çœ‹åˆ°é€™éº¼å¤§ç­†çš„training dataä¸å¤ªæœƒè™•ç†ï¼Œè€Œç”¢ç”Ÿäº†ä¸€ç³»åˆ—çš„å•é¡Œã€‚\n",
    "   (4) å˜—è©¦äº†Random Forestå’ŒXGBoostå…©å€‹æ–¹æ³•ï¼Œä½†æ˜¯æ•ˆæœéƒ½ä¸ç›¡ç†æƒ³ï¼Œåœ¨40%å·¦å³ã€‚\n",
    "   (5) æœ€çµ‚æƒ³äº†ä¸€å€‹æ–¹å¼ï¼Œå°±æ˜¯ä¸åªè€ƒæ…®ä¸€å€‹æ¨¡å‹çš„çµæœï¼Œè€Œæ˜¯ä¹Ÿä½¿ç”¨Random Forestå’ŒXGBoostçš„æ¨¡å‹ï¼Œå› ç‚ºæˆ‘ç™¼ç¾å‰å…©è€…åœ¨joyå’Œanticipationçš„            recall rateæœƒå¿…è¼ƒä½ï¼Œæˆ‘æƒ³åŸå› å¯èƒ½æ˜¯å› ç‚ºä»–å€‘åœ¨é‚£å…©å€‹æƒ…ç·’çš„trainingè³‡æ–™æ¯”è¼ƒå¤šçš„é—œä¿‚ï¼Œæ–¼æ˜¯æˆ‘åœ¨é€™éƒ¨åˆ†ä¿ç•™LSVCçš„ç­”æ¡ˆï¼Œè€Œåœ¨å…¶ä»–çš„æƒ…ç·’åš        æŠ•ç¥¨ï¼Œæœ€çµ‚çµæœçš„æº–ç¢ºç‡æœçœŸä¸Šå‡äº†ä¸€äº›ï¼Œåˆ°é”46.5%ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "# ç¬¬ä¸€éƒ¨åˆ†åœ¨DM2020-Lab2-Master.ipynb\n",
    "# ä»¥ä¸‹æ˜¯ç¬¬äºŒéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_identification = pd.read_csv('dm2020-hw2-nthu/data_identification.csv')\n",
    "emotion = pd.read_csv('dm2020-hw2-nthu/emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸‹æ˜¯å°‡è³‡æ–™å­˜é€²ä¾†ï¼Œé€™è£¡æœƒå°‡è³‡æ–™å­˜æˆDictionaryçš„æ ¼å¼ï¼Œé€™æ˜¯ç‚ºäº†æ–¹ä¾¿ä¹‹å¾Œå°‡trainning setã€data setã€emotionåŠ å…¥åˆ°dataframeä¸­\n",
    "# å› ç‚ºdictionaryæœ‰ç¶“éhashfunctionï¼Œæ‰€ä»¥ä¸æœƒé€ æˆåœ¨åˆä½µ100å¤šè¬ç­†è³‡æ–™èŠ±è¨±å¤šæ™‚é–“\n",
    "data_identification_dict = data_identification.set_index('tweet_id').T.to_dict('list')\n",
    "emotion_dict = emotion.set_index('tweet_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_identification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_identification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_identification[data_identification['identification'] == 'train']))\n",
    "print(len(data_identification[data_identification['identification'] == 'test']))\n",
    "print(len(data_identification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# å°‡jsonæª”æ¡ˆè®€å…¥\n",
    "with open('dm2020-hw2-nthu/tweets_DM.json') as json_file: \n",
    "    tweets_DM_list = []\n",
    "    for line in json_file.readlines():\n",
    "        data = json.loads(line) \n",
    "        tweets_DM_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”ŸDataFrameçš„çµæ§‹\n",
    "tweets_DM = pd.DataFrame(tweets_DM_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = []\n",
    "text = []\n",
    "dataset_list = []\n",
    "emotion_list = []\n",
    "Used_data = tweets_DM.shape[0]\n",
    "\n",
    "# å°‡åŸæœ¬è®€å…¥çš„è³‡æ–™ä½œè§£æï¼Œç”¢ç”Ÿtweet_idå’Œtextçš„column\n",
    "for i in range(Used_data):\n",
    "    tweet_id.append(tweets_DM.loc[i,'_source']['tweet']['tweet_id'])\n",
    "    text.append(tweets_DM.loc[i,'_source']['tweet']['text'])\n",
    "    \n",
    "tweets_DM['tweet_id'] = tweet_id + [np.nan] * (tweets_DM.shape[0] - Used_data)\n",
    "tweets_DM['text'] = text + [np.nan] * (tweets_DM.shape[0] - Used_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€™è£¡å°‡data_identification.csvå’Œemotion.csvçš„è³‡æ–™ä½µå…¥tweets_DMï¼Œç”¨åˆ°Dictionaryæ‰€ä»¥é€Ÿåº¦å¿«å¾ˆå¤š\n",
    "for i in range(Used_data):\n",
    "    temp1 = data_identification_dict.get(tweets_DM.loc[i,'tweet_id'])\n",
    "    temp2 = emotion_dict.get(tweets_DM.loc[i,'tweet_id'])\n",
    "    if temp1 == None:\n",
    "        dataset_list.append(np.nan)\n",
    "    else:\n",
    "        dataset_list.append(temp1[0])\n",
    "    if temp2 == None:\n",
    "        emotion_list.append(np.nan)\n",
    "    else:\n",
    "        emotion_list.append(temp2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡åŸæœ¬ _source æ¬„ä½åˆªé™¤ï¼Œå› ç‚ºä¸éœ€è¦ç”¨åˆ°äº†ï¼Œå·²ç¶“è§£ææˆå…¶ä»–column\n",
    "del tweets_DM['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”Ÿdataset(train or test)å’Œemotion(8ç¨®æƒ…ç·’)çš„column\n",
    "dataset_list = dataset_list + [np.nan] * (tweets_DM.shape[0] - Used_data)\n",
    "emotion_list = emotion_list + [np.nan] * (tweets_DM.shape[0] - Used_data)\n",
    "tweets_DM['dataset'] = dataset_list\n",
    "tweets_DM['emotion'] = emotion_list\n",
    "tweets_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡å‰é¢èŠ±å¾ˆå¤šæ™‚é–“åšè³‡æ–™è®€å–ã€åˆä½µçš„éƒ¨åˆ†å­˜æˆpklæª”æ¡ˆï¼Œæ–¹ä¾¿ä»¥å¾Œå†æ¬¡è®€å–è³‡æ–™\n",
    "tweets_DM.to_pickle(\"tweets_DM.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM = pd.read_pickle(\"tweets_DM.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess_data(data):\n",
    "    lemmatizer = WordNetLemmatizer() # å°‡æ–‡å­—è½‰æˆä»–çš„baseform\n",
    "    data = data.apply(lambda x: re.sub(r'@\\w+',r'@',x)) # ç§»é™¤taggedçš„åå­—      \n",
    "    data = data.apply(lambda x: re.sub(r'<LH>|[,.~\\'â€™\"â€:;&]|http\\S+',' ',x)) # å°‡çœ‹ä¸æ‡‚çš„ç¬¦è™Ÿå’Œå¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿå»é™¤\n",
    "    data = data.apply(lambda x: re.sub(r'#(\\w+)',r'\\1',x)) # å°‡hashtagç§»é™¤\n",
    "    data = data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.lower().strip().split()]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡è³‡æ–™åšå‰è™•ç†\n",
    "tweets_DM['text'] = preprocess_data(tweets_DM['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„è³‡æ–™\n",
    "sum(tweets_DM.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tweets_DM[tweets_DM['dataset'] == 'train']\n",
    "test_df = tweets_DM[tweets_DM['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦æœ‰missingçš„è³‡æ–™\n",
    "train_df.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(n=1000000) #random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group to find distribution\n",
    "train_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df['text']\n",
    "Y = train_df['emotion']\n",
    "\n",
    "# åˆ‡å‡ºtraining setå’Œdata set\n",
    "X_train_origin, X_test_origin, y_train_origin, y_test_origin = train_test_split(X, Y, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at data dimension is a good habbit  :)\n",
    "print('X_train_origin.shape: ', X_train_origin.shape)\n",
    "print('y_train_origin.shape: ', y_train_origin.shape)\n",
    "print('X_test_origin.shape: ', X_test_origin.shape)\n",
    "print('y_test_origin.shape: ', y_test_origin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "### Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build analyzers (bag-of-words)\n",
    "BOW_vectorizer = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(X_train_origin)\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(X_train_origin)\n",
    "test_data_BOW_features = BOW_vectorizer.transform(X_test_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data_BOW_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimension\n",
    "train_data_BOW_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe some feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ğŸ˜‚\" in feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=1000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(X_train_origin)\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(X_train_origin)\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_BOW_features_500.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ğŸ˜‚\" in feature_names_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(X_train_origin)\n",
    "y_train = y_train_origin\n",
    "\n",
    "X_test = BOW_500.transform(X_test_origin)\n",
    "y_test = y_test_origin\n",
    "\n",
    "# take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "Predict_answer_train = mnb.predict(X_train)\n",
    "Predict_answer_test = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_true=y_train, y_pred=Predict_answer_train)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=Predict_answer_test)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€™æ˜¯naive-basedæ–¹æ³•çš„classification_report\n",
    "print(classification_report(y_true=y_test, y_pred=Predict_answer_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(X_train_origin)\n",
    "y_train = y_train_origin\n",
    "\n",
    "X_test = BOW_500.transform(X_test_origin)\n",
    "y_test = y_test_origin\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with categorical label (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# ä»¥ä¸‹æ˜¯ç”¢ç”ŸTrain loss å’Œ Val lossçš„åœ–\n",
    "loss_train = history.history['loss'] \n",
    "loss_val = history.history['val_loss'] \n",
    "epochs = range(0,25) \n",
    "# è¨­å®šåœ–çš„é•·å¯¬\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss_train, 'b', label='Train loss') \n",
    "plt.plot(epochs, loss_val, 'r', label='Val loss') \n",
    "plt.title('Training Loss per epoch') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.legend() \n",
    "\n",
    "# ä»¥ä¸‹æ˜¯ç”¢ç”ŸTrain accuracy å’Œ Val accuracyçš„åœ–\n",
    "acc_train = history.history['accuracy'] \n",
    "acc_val = history.history['val_accuracy'] \n",
    "epochs = range(0,25) \n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc_train, 'b', label='Train accuracy') \n",
    "plt.plot(epochs, acc_val, 'r', label='Val accuracy') \n",
    "plt.title('Training Accuracy per epoch') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1,tokenizer=nltk.word_tokenize)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_origin) \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf_svc = LinearSVC()\n",
    "tfidf_svc.fit(X_train_tfidf, y_train_origin)\n",
    "tfidf_svc_pred_train = tfidf_svc.predict(X_train_tfidf)\n",
    "tfidf_svc_pred_test = tfidf_svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_true=y_train_origin, y_pred=tfidf_svc_pred_train)\n",
    "acc_test = accuracy_score(y_true=y_test_origin, y_pred=tfidf_svc_pred_test)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# text preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_df.groupby(['emotion']).count().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 8 # æœ‰å…«ç¨®æƒ…ç·’\n",
    "embed_num_dims = 300 # æŠŠdimensionè¨­ç‚º300\n",
    "max_seq_len = 500 # é è¨­æœ€é•·çš„é•·åº¦æ˜¯500ï¼Œä¸è¶³è£œ0(zerro padding)ï¼Œè¶…éå‰‡æˆªæ–·\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    # ä½¿ç”¨nltkçš„tokenize\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(clean_text(text)) for text in X_train_origin]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€™è£¡æ˜¯å°LSTMæ‰€éœ€çš„Inputåšäº›è™•ç†\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(X_train_origin)\n",
    "sequence_test = tokenizer.texts_to_sequences(X_test_origin)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "# ä¸åŒå­—æ¯çš„æ•¸é‡ï¼ŒåŠ ä¸€æ˜¯å› ç‚ºæœƒè£œ0çš„é—œä¿‚\n",
    "vocab_size = len(index_of_words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸‹åšpaddingçš„å‹•ä½œ\n",
    "X_submit_origin = test_df['text']\n",
    "sequence_submit = tokenizer.texts_to_sequences(X_submit_origin)\n",
    "X_submit_pad = pad_sequences(sequence_submit, maxlen = max_seq_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
    "\n",
    "X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "     'anger' : 0,\n",
    "     'anticipation' : 1,\n",
    "     'disgust' : 2,\n",
    "     'fear' : 3,\n",
    "     'joy' : 4,\n",
    "     'sadness' : 5,\n",
    "     'surprise' : 6,\n",
    "     'trust' : 7\n",
    "}\n",
    "\n",
    "# å°LSTMçš„y labelåšencodingçš„å‹•ä½œ\n",
    "y_train_lstm = [encoding[x] for x in y_train_origin]\n",
    "y_test_lstm = [encoding[x] for x in y_test_origin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "y_train_lstm = to_categorical(y_train_lstm)\n",
    "y_test_lstm = to_categorical(y_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¢ç”Ÿembedding matrix\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1 \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open(filepath,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ç¶²è·¯ä¸ŠæŠ“wikiçš„word-vector\n",
    "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
    "                              'wiki-news-300d-1M.vec.zip')\n",
    "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('embeddings')\n",
    "    \n",
    "    os.remove('wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
    "embedd_matrix.shape # ç¬¬ä¸€å€‹æ•¸å­—æ˜¯Uniqueå­—æ¯çš„æ•¸é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥æœ‰å¤šå°‘å­—æ¯ä¸åœ¨wiki wordvectorä¸­\n",
    "new_words = 0\n",
    "\n",
    "for word in index_of_words:\n",
    "    entry = embedd_matrix[index_of_words[word]]\n",
    "    if all(v == 0 for v in entry):\n",
    "        new_words = new_words + 1\n",
    "\n",
    "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
    "print('New words found: ' + str(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹LSTMçš„embedding layer\n",
    "embedd_layer = Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = max_seq_len,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gru_output_size = 128\n",
    "bidirectional = True\n",
    "\n",
    "# é †åºç‚ºEmbedding Layer, biLSTM, Dense, softmax\n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "\n",
    "if bidirectional:\n",
    "    model.add(Bidirectional(GRU(units=gru_output_size,\n",
    "                              dropout=0.2,\n",
    "                              recurrent_dropout=0.2)))\n",
    "else:\n",
    "     model.add(GRU(units=gru_output_size,\n",
    "                dropout=0.2, \n",
    "                recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "# Training\n",
    "hist = model.fit(X_train_pad, y_train_lstm, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_test_pad,y_test_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit_origin = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = tfidf_vectorizer.transform(X_submit_origin)\n",
    "print('X_submit.shape: ', X_submit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_answer_submit = tfidf_svc.predict(X_submit)\n",
    "Predict_answer_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = test_df['tweet_id'].to_frame()\n",
    "submit_df = submit_df.rename(columns = {'tweet_id':'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['emotion'] = Predict_answer_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('Result_13.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
